-----

## Scrapy çˆ¬èŸ²å°ˆæ¡ˆ README.md

### å°ˆæ¡ˆç°¡ä»‹

é€™æ˜¯ä¸€å€‹ä½¿ç”¨ Python å’Œ Scrapy æ¡†æ¶ç·¨å¯«çš„ç¶²è·¯çˆ¬èŸ²ï¼Œå°ˆç‚ºæŠ“å– **koc.com.tw** ç¶²ç«™ä¸Šçš„æ–‡ç« å…§å®¹è€Œè¨­è¨ˆã€‚æ­¤çˆ¬èŸ²çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯æ¨¡æ“¬ç¶²ç«™çš„ç„¡é™æ»¾å‹•ï¼ˆInfinite Scrollï¼‰è¡Œç‚ºï¼Œé€éç™¼é€é€£çºŒçš„ **AJAX POST è«‹æ±‚**ä¾†ç²å–æ‰€æœ‰é é¢ä¸Šçš„æ–‡ç« è³‡æ–™ã€‚


-----

### æª”æ¡ˆçµæ§‹

æœ¬å°ˆæ¡ˆåŒ…å«ä»¥ä¸‹ä¸»è¦æª”æ¡ˆï¼š

  - `koc_crawler/spiders/koc.py`: çˆ¬èŸ²çš„ä¸»è¦ç¨‹å¼ç¢¼ï¼Œå®šç¾©äº†æŠ“å–é‚è¼¯ã€‚
  - `koc_crawler/items.py`: å®šç¾©äº†æ–‡ç« è³‡æ–™çš„çµæ§‹ (`ArticleItem`)ã€‚
  - `koc_crawler/settings.py`: å°ˆæ¡ˆçš„è¨­å®šæª”ã€‚

-----

### å¦‚ä½•åŸ·è¡Œ

è«‹ç¢ºèªæ‚¨å·²å®‰è£ Scrapyã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤ä¾†åŸ·è¡Œçˆ¬èŸ²ï¼Œä¸¦å°‡æŠ“å–åˆ°çš„è³‡æ–™è¼¸å‡ºç‚º JSON æˆ– CSV æª”æ¡ˆï¼š

**è¼¸å‡ºç‚º JSON æ ¼å¼ï¼š**

```
scrapy crawl koc -o articles.json
```

**è¼¸å‡ºç‚º CSV æ ¼å¼ï¼š**

```
scrapy crawl koc -o articles.csv
```

-----

### è¨­å®šèˆ‡å®¢è£½åŒ–

æ‚¨å¯ä»¥é€éä¿®æ”¹ `koc.py` æª”æ¡ˆä¸­çš„ `MAX_PAGES` è®Šæ•¸ä¾†æ§åˆ¶çˆ¬èŸ²è¦æŠ“å–å¤šå°‘é ã€‚
ä¾‹å¦‚ï¼Œè‹¥è¦é™åˆ¶åªæŠ“å– 5 é ï¼Œè«‹å°‡ `MAX_PAGES` è¨­ç‚º `5`ï¼š

```python
class KocSpider(scrapy.Spider):
    ...
    MAX_PAGES = 5  # è¨­å®šæ‚¨æƒ³è¦æŠ“å–çš„æœ€å¤§é æ•¸
    ...
```

è‹¥æ‚¨å¸Œæœ›æŠ“å–æ‰€æœ‰é é¢ï¼Œå¯ä»¥å°‡ `MAX_PAGES` è¨­å®šç‚ºä¸€å€‹è¶³å¤ å¤§çš„æ•¸å­—ï¼Œæˆ–ç›´æ¥ç§»é™¤é€™å€‹é æ•¸é™åˆ¶çš„æ¢ä»¶åˆ¤æ–·ã€‚

-----

### æŠ€è¡“ç´°ç¯€

æ­¤çˆ¬èŸ²çš„é‹ä½œæ–¹å¼ä¸¦éå‚³çµ±çš„è·Ÿéš¨è¶…é€£çµã€‚å®ƒæ¨¡æ“¬äº†ç¶²ç«™çš„ç„¡é™æ»¾å‹•æ©Ÿåˆ¶ï¼Œé€éå‘ç¶²ç«™çš„ AJAX API (`https://www.koc.com.tw/?ajax-request=jnews`) ç™¼é€ POST è«‹æ±‚ä¾†ç²å–å…§å®¹ã€‚

æ¯æ¬¡è«‹æ±‚éƒ½æœƒåœ¨ `body` ä¸­å¸¶ä¸Š `data[current_page]` åƒæ•¸ï¼Œä¸¦åœ¨æ¯æ¬¡æˆåŠŸæŠ“å–å¾Œè‡ªå‹•éå¢ï¼Œä»¥å¯¦ç¾é é¢çš„é€£çºŒæŠ“å–ã€‚ä¼ºæœå™¨å›å‚³çš„æ˜¯åŒ…å« HTML å…§å®¹çš„ JSON è³‡æ–™ï¼Œçˆ¬èŸ²æœƒè§£æé€™äº›è³‡æ–™ä¾†æå–æ‰€éœ€çš„æ–‡ç« è³‡è¨Šã€‚



---
# ğŸŸ¢ å®Œæ•´æµç¨‹ï¼ˆç„¡åˆ†å²”ï¼‰

## 1. å»ºç«‹ Scrapy å°ˆæ¡ˆ

åœ¨ VSCode çµ‚ç«¯æ©Ÿï¼ˆå·²å•Ÿç”¨è™›æ“¬ç’°å¢ƒï¼‰è¼¸å…¥ï¼š

```powershell
scrapy startproject koc_crawler .
```

å°ˆæ¡ˆçµæ§‹æœƒé•·é€™æ¨£ï¼š

```
koc_crawler/
    spiders/
        __init__.py
scrapy.cfg
```

---

## 2. å®šç¾©è³‡æ–™çµæ§‹

æ‰“é–‹ `koc_crawler/items.py`ï¼Œæ”¹æˆï¼š

```python
import scrapy

class ArticleItem(scrapy.Item):
    url = scrapy.Field()
    title = scrapy.Field()
    author = scrapy.Field()
    published_at = scrapy.Field()
    content = scrapy.Field()
```

---

## 3. ä¿®æ”¹è¨­å®š

æ‰“é–‹ `koc_crawler/settings.py`ï¼Œç›´æ¥åŠ ä¸Šæˆ–ä¿®æ”¹é€™å¹¾è¡Œï¼š

```python
ROBOTSTXT_OBEY = False
FEED_EXPORT_ENCODING = "utf-8"

FEEDS = {
    "data/koc.jsonl": {"format": "jsonlines"},
}

DEFAULT_REQUEST_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36"
}

DOWNLOAD_DELAY = 0.5
DEPTH_LIMIT = 2
LOG_LEVEL = "INFO"
```

---

## 4. å»ºç«‹ Spider

æ–°å¢æª”æ¡ˆ `koc_crawler/spiders/blog_spider.py`ï¼Œå…§å®¹å®Œæ•´å¦‚ä¸‹ï¼š

```python
import scrapy
from urllib.parse import urljoin
from koc_crawler.items import ArticleItem

class BlogSpider(scrapy.Spider):
    name = "koc"
    allowed_domains = ["koc.com.tw", "www.koc.com.tw"]
    start_urls = ["https://www.koc.com.tw/"]

    def parse(self, response):
        # æ‰¾æ–‡ç« é€£çµï¼ˆæ­¤ç‚ºç¯„ä¾‹ï¼Œå¯¦éš›ä¸Šå¯ä¾ç¶²ç«™çµæ§‹èª¿æ•´ï¼‰
        article_links = response.css("a::attr(href)").getall()
        for href in article_links:
            url = urljoin(response.url, href)
            if any(domain in url for domain in self.allowed_domains):
                yield scrapy.Request(url, callback=self.parse_article)

    def parse_article(self, response):
        item = ArticleItem()
        item["url"] = response.url
        item["title"] = response.css("title::text").get(default="").strip()
        item["author"] = response.css(".author::text, .post-author::text").get(default="").strip()
        item["published_at"] = response.css("time::attr(datetime), .date::text").get(default="").strip()
        paragraphs = response.css("p::text").getall()
        item["content"] = "\n".join([p.strip() for p in paragraphs if p.strip()])
        yield item
```

---

## 5. åŸ·è¡Œçˆ¬èŸ²

åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼ˆæœ‰ `scrapy.cfg` çš„åœ°æ–¹ï¼‰åŸ·è¡Œï¼š

```powershell
scrapy crawl koc
```

åŸ·è¡Œå¾Œï¼Œçµæœæœƒå­˜åˆ°ï¼š

```
data/koc.jsonl
```

ä½ å¯ä»¥ç”¨ VSCode æˆ–è¨˜äº‹æœ¬æ‰“é–‹ä¾†çœ‹ï¼Œå…§å®¹æ˜¯ä¸€è¡Œä¸€ç¯‡æ–‡ç« çš„ JSONã€‚

---

